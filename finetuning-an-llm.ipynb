{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":5283213,"sourceType":"datasetVersion","datasetId":3009785},{"sourceId":7370207,"sourceType":"datasetVersion","datasetId":4282068},{"sourceId":6076,"sourceType":"modelInstanceVersion","modelInstanceId":4695,"modelId":2823}],"dockerImageVersionId":30746,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-08-08T19:04:34.900296Z","iopub.execute_input":"2024-08-08T19:04:34.900650Z","iopub.status.idle":"2024-08-08T19:04:35.942686Z","shell.execute_reply.started":"2024-08-08T19:04:34.900619Z","shell.execute_reply":"2024-08-08T19:04:35.941652Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"/kaggle/input/chatbot-ai-q-and-a/AI.parquet\n/kaggle/input/chatbot-ai-q-and-a/AI.csv\n/kaggle/input/python-dataset/Python codes.csv\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/config.json\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/tokenizer.json\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/metadata.json\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/model.weights.h5\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/assets/tokenizer/merges.txt\n/kaggle/input/gpt2/keras/gpt2_medium_en/2/assets/tokenizer/vocabulary.json\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers datasets evaluate accelerate peft","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:04:39.163636Z","iopub.execute_input":"2024-08-08T19:04:39.164089Z","iopub.status.idle":"2024-08-08T19:04:53.448595Z","shell.execute_reply.started":"2024-08-08T19:04:39.164062Z","shell.execute_reply":"2024-08-08T19:04:53.447495Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.10/site-packages (4.42.3)\nRequirement already satisfied: datasets in /opt/conda/lib/python3.10/site-packages (2.20.0)\nCollecting evaluate\n  Downloading evaluate-0.4.2-py3-none-any.whl.metadata (9.3 kB)\nRequirement already satisfied: accelerate in /opt/conda/lib/python3.10/site-packages (0.32.1)\nCollecting peft\n  Downloading peft-0.12.0-py3-none-any.whl.metadata (13 kB)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.10/site-packages (from transformers) (3.13.1)\nRequirement already satisfied: huggingface-hub<1.0,>=0.23.2 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.23.4)\nRequirement already satisfied: numpy<2.0,>=1.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (1.26.4)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.10/site-packages (from transformers) (21.3)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (6.0.1)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.10/site-packages (from transformers) (2023.12.25)\nRequirement already satisfied: requests in /opt/conda/lib/python3.10/site-packages (from transformers) (2.32.3)\nRequirement already satisfied: safetensors>=0.4.1 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.4.3)\nRequirement already satisfied: tokenizers<0.20,>=0.19 in /opt/conda/lib/python3.10/site-packages (from transformers) (0.19.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.10/site-packages (from transformers) (4.66.4)\nRequirement already satisfied: pyarrow>=15.0.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (16.1.0)\nRequirement already satisfied: pyarrow-hotfix in /opt/conda/lib/python3.10/site-packages (from datasets) (0.6)\nRequirement already satisfied: dill<0.3.9,>=0.3.0 in /opt/conda/lib/python3.10/site-packages (from datasets) (0.3.8)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.10/site-packages (from datasets) (2.2.2)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.10/site-packages (from datasets) (3.4.1)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.10/site-packages (from datasets) (0.70.16)\nRequirement already satisfied: fsspec<=2024.5.0,>=2023.1.0 in /opt/conda/lib/python3.10/site-packages (from fsspec[http]<=2024.5.0,>=2023.1.0->datasets) (2024.5.0)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.10/site-packages (from datasets) (3.9.1)\nRequirement already satisfied: psutil in /opt/conda/lib/python3.10/site-packages (from accelerate) (5.9.3)\nRequirement already satisfied: torch>=1.10.0 in /opt/conda/lib/python3.10/site-packages (from accelerate) (2.1.2)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (23.2.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (6.0.4)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.9.3)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.4.1)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (1.3.1)\nRequirement already satisfied: async-timeout<5.0,>=4.0 in /opt/conda/lib/python3.10/site-packages (from aiohttp->datasets) (4.0.3)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.10/site-packages (from huggingface-hub<1.0,>=0.23.2->transformers) (4.9.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.10/site-packages (from packaging>=20.0->transformers) (3.1.1)\nRequirement already satisfied: charset-normalizer<4,>=2 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.3.2)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (3.6)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (1.26.18)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.10/site-packages (from requests->transformers) (2024.7.4)\nRequirement already satisfied: sympy in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (1.13.0)\nRequirement already satisfied: networkx in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.2.1)\nRequirement already satisfied: jinja2 in /opt/conda/lib/python3.10/site-packages (from torch>=1.10.0->accelerate) (3.1.2)\nRequirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.3.post1)\nRequirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.10/site-packages (from pandas->datasets) (2023.4)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.10/site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.16.0)\nRequirement already satisfied: MarkupSafe>=2.0 in /opt/conda/lib/python3.10/site-packages (from jinja2->torch>=1.10.0->accelerate) (2.1.3)\nRequirement already satisfied: mpmath<1.4,>=1.1.0 in /opt/conda/lib/python3.10/site-packages (from sympy->torch>=1.10.0->accelerate) (1.3.0)\nDownloading evaluate-0.4.2-py3-none-any.whl (84 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m84.1/84.1 kB\u001b[0m \u001b[31m5.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading peft-0.12.0-py3-none-any.whl (296 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.4/296.4 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hInstalling collected packages: peft, evaluate\nSuccessfully installed evaluate-0.4.2 peft-0.12.0\n","output_type":"stream"}]},{"cell_type":"code","source":"import torch\nfrom transformers import GPT2Tokenizer, GPT2LMHeadModel\nimport numpy as np\n\nimport logging\nlogging.getLogger().setLevel(logging.CRITICAL)\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\ndevice = 'cpu'\nprint(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:05:30.676647Z","iopub.execute_input":"2024-08-08T19:05:30.677420Z","iopub.status.idle":"2024-08-08T19:05:30.684327Z","shell.execute_reply.started":"2024-08-08T19:05:30.677372Z","shell.execute_reply":"2024-08-08T19:05:30.683353Z"},"trusted":true},"execution_count":4,"outputs":[{"name":"stdout","text":"cpu\n","output_type":"stream"}]},{"cell_type":"code","source":"tokenizer = GPT2Tokenizer.from_pretrained('gpt2-medium')\nmodel = GPT2LMHeadModel.from_pretrained('gpt2-medium')\nmodel = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:05:33.263850Z","iopub.execute_input":"2024-08-08T19:05:33.264713Z","iopub.status.idle":"2024-08-08T19:06:18.055838Z","shell.execute_reply.started":"2024-08-08T19:05:33.264669Z","shell.execute_reply":"2024-08-08T19:06:18.055039Z"},"trusted":true},"execution_count":5,"outputs":[{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json:   0%|          | 0.00/26.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e49155b87a404cd98903e7bce56c9eac"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"vocab.json:   0%|          | 0.00/1.04M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e7041a54988d4ee89e6922743e1b281a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df6e6be2cbd9427c918c4dbe682e8962"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"002013e2c2254c7487dc885840587757"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json:   0%|          | 0.00/718 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4bf40b9b02534ac8a1c1128e98918165"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model.safetensors:   0%|          | 0.00/1.52G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"399094df2e94461bb441a0ec48c763e6"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/124 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a27999dc67c64eba901b0f2444340062"}},"metadata":{}}]},{"cell_type":"code","source":"from torch.utils.data import Dataset\nfrom torch.utils.data import Dataset, DataLoader\nimport os\nimport json\nimport csv\n\nclass pythonDataset(Dataset):\n    def __init__(self, jokes_dataset_path = '/kaggle/input/chatbot-ai-q-and-a'):\n        super().__init__()\n\n        short_jokes_path = os.path.join(jokes_dataset_path, 'AI.csv')\n\n        self.joke_list = []\n        self.end_of_text_token = \"<|endoftext|>\"\n        \n        with open(short_jokes_path) as csv_file:\n            csv_reader = csv.reader(csv_file, delimiter=',')\n              \n            next(csv_reader, None) \n            \n            for row in csv_reader:\n                # Check if row has the expected number of columns\n                if len(row) > 1:\n                    joke_str = f\"data:{row[1]}{self.end_of_text_token}\"\n                    self.joke_list.append(joke_str)\n        \n    def __len__(self):\n        return len(self.joke_list)\n\n    def __getitem__(self, item):\n        return self.joke_list[item]","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:06:30.973651Z","iopub.execute_input":"2024-08-08T19:06:30.974438Z","iopub.status.idle":"2024-08-08T19:06:30.982724Z","shell.execute_reply.started":"2024-08-08T19:06:30.974404Z","shell.execute_reply":"2024-08-08T19:06:30.981729Z"},"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"dataset = pythonDataset()\ndata_loader = DataLoader(dataset, batch_size=32, shuffle=True)\n","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:06:34.387366Z","iopub.execute_input":"2024-08-08T19:06:34.387738Z","iopub.status.idle":"2024-08-08T19:06:34.403109Z","shell.execute_reply.started":"2024-08-08T19:06:34.387708Z","shell.execute_reply":"2024-08-08T19:06:34.402206Z"},"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"code","source":"BATCH_SIZE = 16\nEPOCHS = 5\nLEARNING_RATE = 1e-6\nWARMUP_STEPS = 500\nMAX_SEQ_LEN = 40","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:06:36.152669Z","iopub.execute_input":"2024-08-08T19:06:36.153290Z","iopub.status.idle":"2024-08-08T19:06:36.157688Z","shell.execute_reply.started":"2024-08-08T19:06:36.153258Z","shell.execute_reply":"2024-08-08T19:06:36.156722Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"from transformers import AdamW, get_linear_schedule_with_warmup\n\nmodel.train()\noptimizer = AdamW(model.parameters(), lr=LEARNING_RATE)\nnum_training_steps = len(data_loader) * EPOCHS\nscheduler = get_linear_schedule_with_warmup(optimizer, num_warmup_steps=WARMUP_STEPS, num_training_steps=num_training_steps)\n\nproc_seq_count = 0\nsum_loss = 0.0\nbatch_count = 0\n\ntmp_paragraphs_tens = None\nmodels_folder = \"/kaggle/working/trained_models\"  # folder for the trained model\n\nif not os.path.exists(models_folder):\n    os.mkdir(models_folder)\n\nfor epoch in range(EPOCHS):\n    \n    print(f\"EPOCH {epoch} started\" + '=' * 30)\n    \n    for idx, paragraph in enumerate(data_loader):\n        \n        #################### Fit as many paragraph sequences into MAX_SEQ_LEN sequence as possible ####\n        paragraph_tens = torch.tensor(tokenizer.encode(paragraph[0])).unsqueeze(0).to(device)\n        \n        if paragraph_tens.size()[1] > MAX_SEQ_LEN:\n            paragraph_tens = paragraph_tens[:, :MAX_SEQ_LEN]\n        \n        if not torch.is_tensor(tmp_paragraphs_tens):\n            tmp_paragraphs_tens = paragraph_tens\n            continue\n        else:\n            if tmp_paragraphs_tens.size()[1] + paragraph_tens.size()[1] > MAX_SEQ_LEN:\n                work_paragraphs_tens = tmp_paragraphs_tens\n                tmp_paragraphs_tens = paragraph_tens\n            else:\n                tmp_paragraphs_tens = torch.cat([tmp_paragraphs_tens, paragraph_tens[:, 1:]], dim=1)\n                continue\n                \n        outputs = model(work_paragraphs_tens, labels=work_paragraphs_tens)\n        loss, logits = outputs[:2]                        \n        loss.backward()\n        sum_loss += loss.detach().data\n                       \n        proc_seq_count += 1\n        if proc_seq_count == BATCH_SIZE:\n            proc_seq_count = 0    \n            batch_count += 1\n            optimizer.step()\n            scheduler.step() \n            optimizer.zero_grad()\n            model.zero_grad()\n\n        if batch_count == 100:\n            print(f\"sum loss is {sum_loss}\")\n            batch_count = 0\n            sum_loss = 0.0\n    \n    torch.save(model.state_dict(), os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{epoch}.pt\"))","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:08:02.726417Z","iopub.execute_input":"2024-08-08T19:08:02.727280Z","iopub.status.idle":"2024-08-08T19:09:07.311678Z","shell.execute_reply.started":"2024-08-08T19:08:02.727239Z","shell.execute_reply":"2024-08-08T19:09:07.310881Z"},"trusted":true},"execution_count":11,"outputs":[{"name":"stdout","text":"EPOCH 0 started==============================\nEPOCH 1 started==============================\nEPOCH 2 started==============================\nEPOCH 3 started==============================\nEPOCH 4 started==============================\n","output_type":"stream"}]},{"cell_type":"code","source":"def choose_from_top(probs, n=2, random_seed=None):\n    ind = np.argpartition(probs, -n)[-n:]\n    top_prob = probs[ind]\n    top_prob = top_prob / np.sum(top_prob) # Normalize\n    np.random.seed(random_seed)\n    choice = np.random.choice(n, 1, p = top_prob)\n    token_id = ind[choice][0]\n    return int(token_id)","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:09:12.570785Z","iopub.execute_input":"2024-08-08T19:09:12.571430Z","iopub.status.idle":"2024-08-08T19:09:12.577669Z","shell.execute_reply.started":"2024-08-08T19:09:12.571397Z","shell.execute_reply":"2024-08-08T19:09:12.576603Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"MODEL_EPOCH = 4\nmodel_path = os.path.join(models_folder, f\"gpt2_medium_pythonlecturer_{MODEL_EPOCH}.pt\")\nmodel.load_state_dict(torch.load(model_path))\n\ngenerated_paragraph = 5\nfirst_sentence = \"Q:Who did the first work generally recognized as AI? \\n\\n A:\"\nmax_paragraph_length = 100\n\nparagraphs_output_file_path = os.path.join(\"/kaggle/working/\", f'generated_lectures_{MODEL_EPOCH}.txt')\naudio_file_folder = \"/kaggle/working/\"\nif os.path.exists(paragraphs_output_file_path):\n    os.remove(paragraphs_output_file_path)\n\nrandomness = None\nmodel.eval()\nwith torch.no_grad():\n    for paragraph_idx in range(generated_paragraph):\n        paragraph_finished = False\n        cur_ids = torch.tensor(tokenizer.encode(first_sentence)).unsqueeze(0).to(device)\n        for i in range(max_paragraph_length):\n            outputs = model(cur_ids, labels=cur_ids)\n            loss, logits = outputs[:2]\n            softmax_logits = torch.softmax(logits[0, -1], dim=0)\n            if i < 3:\n                n = 20\n            else:\n                n = 3\n                \n            next_token_id = choose_from_top(softmax_logits.to('cpu').numpy(), n=n, random_seed=randomness)\n\n            cur_ids = torch.cat([cur_ids, torch.ones((1, 1)).long().to(device) * next_token_id], dim=1)\n\n            if next_token_id in tokenizer.encode('.'):\n                paragraph_finished = True\n                break\n\n        if paragraph_finished:\n            output_list = list(cur_ids.squeeze().to('cpu').numpy())\n            output_text = tokenizer.decode(output_list)\n            print(output_text + \"\\n\")\n\n            with open(paragraphs_output_file_path, 'a') as f:\n                f.write(f\"{output_text} \\n\\n\")","metadata":{"execution":{"iopub.status.busy":"2024-08-08T19:09:14.538746Z","iopub.execute_input":"2024-08-08T19:09:14.539104Z","iopub.status.idle":"2024-08-08T19:09:55.769524Z","shell.execute_reply.started":"2024-08-08T19:09:14.539074Z","shell.execute_reply":"2024-08-08T19:09:55.768542Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"2024-08-08 19:09:20.464709: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:9261] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n2024-08-08 19:09:20.464817: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:607] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2024-08-08 19:09:20.586230: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1515] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"Q:Who did the first work generally recognized as AI? \n\n A:I don't know, but it's a very important question.\n\nQ:Who did the first work generally recognized as AI? \n\n A:I don't think anyone really has any clue who did it, but the earliest known mention of it is in 1878 by the German physicist, Carl von Löw.\n\nQ:Who did the first work generally recognized as AI? \n\n A:I think we have to look back at the history of the field.\n\nQ:Who did the first work generally recognized as AI? \n\n A:The first work that was recognized as AI was the work of the German mathematician and philosopher Gottfried Wilhelm Leibniz, who published the first formal definition of AI in his book The Theory of Machines.\n\nQ:Who did the first work generally recognized as AI? \n\n A:Babbage.\n\n","output_type":"stream"}]}]}